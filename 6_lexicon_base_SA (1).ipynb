{"metadata":{"kernelspec":{"name":"xpython","display_name":"Python 3.7 (XPython)","language":"python"},"language_info":{"file_extension":".py","mimetype":"text/x-python","name":"python","version":"3.7.9"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### SA with Vader\n\nIn their official [repo](https://github.com/cjhutto/vaderSentiment) you can find the lexicon, a list of rules and the support ofr emoji recognition","metadata":{}},{"cell_type":"code","source":"!pip install vaderSentiment  # Vader √® una repository :ha un lexicon, cio√® 7520 parole \n                             # divise in gruppi secondo la direzione di sentimento","metadata":{"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting vaderSentiment\n  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125 kB 4.2 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: requests in /srv/conda/envs/notebook/lib/python3.7/site-packages (from vaderSentiment) (2.25.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests->vaderSentiment) (1.26.3)\nRequirement already satisfied: chardet<5,>=3.0.2 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests->vaderSentiment) (3.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests->vaderSentiment) (2020.12.5)\nRequirement already satisfied: idna<3,>=2.5 in /srv/conda/envs/notebook/lib/python3.7/site-packages (from requests->vaderSentiment) (2.10)\nInstalling collected packages: vaderSentiment\nSuccessfully installed vaderSentiment-3.3.2\n","output_type":"stream"}]},{"cell_type":"code","source":"sentences=[\"I like text mining so much, it is super fun\", \n           \"I can't bear this lecture anymore\", \n           \"today is quite sunny outside\",\n           \"I hate flamingos, they're so inappropriate\",\n           \"Catch utf-8 emoji such as üíò and üíã and üòÅ\"]","metadata":{"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\nanalyzer = SentimentIntensityAnalyzer()\nfor sentence in sentences:\n    vs = analyzer.polarity_scores(sentence)\n    print(\"{:-<50} {}\".format(sentence, str(vs)))  # compound:somma del valore delle parole, normalizzata tra -1 e 1.","metadata":{"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"I like text mining so much, it is super fun------- {'neg': 0.0, 'neu': 0.419, 'pos': 0.581, 'compound': 0.8658}\nI can't bear this lecture anymore----------------- {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}\ntoday is quite sunny outside---------------------- {'neg': 0.0, 'neu': 0.564, 'pos': 0.436, 'compound': 0.4754}\nI hate flamingos, they're so inappropriate-------- {'neg': 0.425, 'neu': 0.575, 'pos': 0.0, 'compound': -0.5719}\nCatch utf-8 emoji such as üíò and üíã and üòÅ----------- {'neg': 0.0, 'neu': 0.583, 'pos': 0.417, 'compound': 0.875}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"emoji [cheat sheet](https://unicode.org/emoji/charts/emoji-list.html)  (replace _U+_ with _U000_)","metadata":{}},{"cell_type":"code","source":"print(\"\\U0001F600\")","metadata":{"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"üòÄ\n","output_type":"stream"}]},{"cell_type":"markdown","source":"[here](https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/emoji_utf8_lexicon.txt) you can find the vader emoticons evaluations","metadata":{}},{"cell_type":"markdown","source":"### About the scoring","metadata":{}},{"cell_type":"markdown","source":"The Positive, Negative and Neutral scores represent the proportion of text that falls in these categories. This means our first sentence was rated as 58% Positive, 42% Neutral and 0% Negative. Hence all these should add up to 1.","metadata":{}},{"cell_type":"markdown","source":"The compound score is computed by summing the valence scores of each word in the lexicon, adjusted according to the rules, and then normalized to be between -1 (most extreme negative) and +1 (most extreme positive). This is the most useful metric if you want a single unidimensional measure of sentiment for a given sentence. Calling it a 'normalized, weighted composite score' is accurate.\n\nIt is also useful for researchers who would like to set standardized thresholds for classifying sentences as either positive, neutral, or negative. Typical threshold values (used in the literature cited on this page) are:\n\npositive sentiment: compound score >= 0.05 <br>\nneutral sentiment: (compound score > -0.05) and (compound score < 0.05) <br>\nnegative sentiment: compound score <= -0.05","metadata":{}},{"cell_type":"markdown","source":"__[Some rules](https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vaderSentiment.py):__  <br>\n\n\n<img src='img/vader_rules.PNG'>","metadata":{}},{"cell_type":"markdown","source":"__Get words polarity:__\n    \nIn the [lexicon](https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vader_lexicon.txt) you can find, for each word in the vocab (7520 elements): <br>\nToken, mean value, SD and scores of 10 independent human evaluators <br>\n\n<img src='img/vader_lexicon.PNG'>\n\n","metadata":{}},{"cell_type":"markdown","source":"### Polarity detection of Amazon data with Vader","metadata":{}},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(\"amazon_cellphones_binary.csv\")\ndf","metadata":{"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>asin</th>\n      <th>reviewText</th>\n      <th>overall</th>\n      <th>bin_y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>B007D6J64K</td>\n      <td>probably favorite cover super sassy protective...</td>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>B007D6J64K</td>\n      <td>case_protects phone damage</td>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>B007D6J64K</td>\n      <td>nice</td>\n      <td>4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>B007D6J64K</td>\n      <td>wa another favorite one thanks quality pricing</td>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>B007D6J64K</td>\n      <td>decent_case lot_of_protection</td>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>26840</th>\n      <td>B0096QI0QK</td>\n      <td>easy_to_put phone protects really_well obvious...</td>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>26841</th>\n      <td>B0096QI0QK</td>\n      <td>much_better quality expected price great_alter...</td>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>26842</th>\n      <td>B0096QI0QK</td>\n      <td>one_of_the_best screen_protector used applicat...</td>\n      <td>4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>26843</th>\n      <td>B0096QI0QK</td>\n      <td>kit included microfiber_cloth soft card help a...</td>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>26844</th>\n      <td>B0096QI0QK</td>\n      <td>wa easiest screen_protector put past bubble sc...</td>\n      <td>5</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>26845 rows √ó 4 columns</p>\n</div>","text/plain":"             asin  ... bin_y\n0      B007D6J64K  ...     1\n1      B007D6J64K  ...     1\n2      B007D6J64K  ...     1\n3      B007D6J64K  ...     1\n4      B007D6J64K  ...     1\n...           ...  ...   ...\n26840  B0096QI0QK  ...     1\n26841  B0096QI0QK  ...     1\n26842  B0096QI0QK  ...     1\n26843  B0096QI0QK  ...     1\n26844  B0096QI0QK  ...     1\n\n[26845 rows x 4 columns]"},"metadata":{}}]},{"cell_type":"code","source":"df = df[~df['reviewText'].isna()]   # eliminiamo le righe vuote","metadata":{"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"compute polarity scores","metadata":{}},{"cell_type":"code","source":"df['scores'] = df['reviewText'].apply(lambda review: analyzer.polarity_scores(review))\ndf.head()\n# df['prova'] = df['ovarall'] + df['bin_y'] ... creiamo nuove colonne in questo modo","metadata":{"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>asin</th>\n      <th>reviewText</th>\n      <th>overall</th>\n      <th>bin_y</th>\n      <th>scores</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>B007D6J64K</td>\n      <td>probably favorite cover super sassy protective...</td>\n      <td>5</td>\n      <td>1</td>\n      <td>{'neg': 0.149, 'neu': 0.532, 'pos': 0.319, 'co...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>B007D6J64K</td>\n      <td>case_protects phone damage</td>\n      <td>5</td>\n      <td>1</td>\n      <td>{'neg': 0.615, 'neu': 0.385, 'pos': 0.0, 'comp...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>B007D6J64K</td>\n      <td>nice</td>\n      <td>4</td>\n      <td>1</td>\n      <td>{'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>B007D6J64K</td>\n      <td>wa another favorite one thanks quality pricing</td>\n      <td>5</td>\n      <td>1</td>\n      <td>{'neg': 0.0, 'neu': 0.459, 'pos': 0.541, 'comp...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>B007D6J64K</td>\n      <td>decent_case lot_of_protection</td>\n      <td>5</td>\n      <td>1</td>\n      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"         asin  ...                                             scores\n0  B007D6J64K  ...  {'neg': 0.149, 'neu': 0.532, 'pos': 0.319, 'co...\n1  B007D6J64K  ...  {'neg': 0.615, 'neu': 0.385, 'pos': 0.0, 'comp...\n2  B007D6J64K  ...  {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound...\n3  B007D6J64K  ...  {'neg': 0.0, 'neu': 0.459, 'pos': 0.541, 'comp...\n4  B007D6J64K  ...  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...\n\n[5 rows x 5 columns]"},"metadata":{}}]},{"cell_type":"markdown","source":"Extract the compund value","metadata":{}},{"cell_type":"code","source":"df['compound']  = df['scores'].apply(lambda score_dict: score_dict['compound']) \ndf.head()    # teniamo solo compound perch√® esprime la polarit√† della frase","metadata":{"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>asin</th>\n      <th>reviewText</th>\n      <th>overall</th>\n      <th>bin_y</th>\n      <th>scores</th>\n      <th>compound</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>B007D6J64K</td>\n      <td>probably favorite cover super sassy protective...</td>\n      <td>5</td>\n      <td>1</td>\n      <td>{'neg': 0.149, 'neu': 0.532, 'pos': 0.319, 'co...</td>\n      <td>0.5859</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>B007D6J64K</td>\n      <td>case_protects phone damage</td>\n      <td>5</td>\n      <td>1</td>\n      <td>{'neg': 0.615, 'neu': 0.385, 'pos': 0.0, 'comp...</td>\n      <td>-0.4939</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>B007D6J64K</td>\n      <td>nice</td>\n      <td>4</td>\n      <td>1</td>\n      <td>{'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound...</td>\n      <td>0.4215</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>B007D6J64K</td>\n      <td>wa another favorite one thanks quality pricing</td>\n      <td>5</td>\n      <td>1</td>\n      <td>{'neg': 0.0, 'neu': 0.459, 'pos': 0.541, 'comp...</td>\n      <td>0.7096</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>B007D6J64K</td>\n      <td>decent_case lot_of_protection</td>\n      <td>5</td>\n      <td>1</td>\n      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n      <td>0.0000</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"         asin  ... compound\n0  B007D6J64K  ...   0.5859\n1  B007D6J64K  ...  -0.4939\n2  B007D6J64K  ...   0.4215\n3  B007D6J64K  ...   0.7096\n4  B007D6J64K  ...   0.0000\n\n[5 rows x 6 columns]"},"metadata":{}}]},{"cell_type":"markdown","source":"compute binary predictions","metadata":{}},{"cell_type":"code","source":"df['predicted'] = df['compound'].apply(lambda c: '1' if c >=0 else '0')\ndf.head()    # predicted tra sentimento positivo :1 e negativo:0","metadata":{"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>asin</th>\n      <th>reviewText</th>\n      <th>overall</th>\n      <th>bin_y</th>\n      <th>scores</th>\n      <th>compound</th>\n      <th>predicted</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>B007D6J64K</td>\n      <td>probably favorite cover super sassy protective...</td>\n      <td>5</td>\n      <td>1</td>\n      <td>{'neg': 0.149, 'neu': 0.532, 'pos': 0.319, 'co...</td>\n      <td>0.5859</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>B007D6J64K</td>\n      <td>case_protects phone damage</td>\n      <td>5</td>\n      <td>1</td>\n      <td>{'neg': 0.615, 'neu': 0.385, 'pos': 0.0, 'comp...</td>\n      <td>-0.4939</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>B007D6J64K</td>\n      <td>nice</td>\n      <td>4</td>\n      <td>1</td>\n      <td>{'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound...</td>\n      <td>0.4215</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>B007D6J64K</td>\n      <td>wa another favorite one thanks quality pricing</td>\n      <td>5</td>\n      <td>1</td>\n      <td>{'neg': 0.0, 'neu': 0.459, 'pos': 0.541, 'comp...</td>\n      <td>0.7096</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>B007D6J64K</td>\n      <td>decent_case lot_of_protection</td>\n      <td>5</td>\n      <td>1</td>\n      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n      <td>0.0000</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"         asin  ... predicted\n0  B007D6J64K  ...         1\n1  B007D6J64K  ...         0\n2  B007D6J64K  ...         1\n3  B007D6J64K  ...         1\n4  B007D6J64K  ...         1\n\n[5 rows x 7 columns]"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.metrics import classification_report\n\nprint(classification_report(df.bin_y, df.predicted.astype('int64'))) \n\n# BoW e Rf accuracy era pari a 0.44 circa, qui √® 0.82: rispetto al deep learning √® per√≤ pi√π bassa, infatti l√¨ era\n# pari a 0.89. Qui non abbiamo per√≤ fatto nessun tipo di training\n# recall (sensitivity) := quanti sono stati classificati correttamente di quelli di classe j\n# precision := \n# macro avg := media delle due sopra (quando abbiamo classi sbilanciate la si usa)\n# weighted avg := media pesata dei 2 sopra per le numerosit√† delle 2 classi","metadata":{"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"              precision    recall  f1-score   support\n\n           0       0.36      0.34      0.35      3754\n           1       0.89      0.90      0.89     21964\n\n    accuracy                           0.82     25718\n   macro avg       0.63      0.62      0.62     25718\nweighted avg       0.81      0.82      0.81     25718\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}